{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning is a branch of Machine Learning where algorithms are written which mimic the functioning of a human brain. The most commonly used libraries in deep learning are Tensorflow and PyTorch. As there are various deep learning frameworks available, one might wonder when to use PyTorch. Here are reasons why one might prefer using Pytorch for specific tasks.\n",
    "\n",
    "### Pytorch is an open-source deep learning framework available with a Python and C++ interface. Pytorch resides inside the torch module. In PyTorch, the data that has to be processed is input in the form of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt # Visualization \n",
    "import matplotlib.dates as mdates # Formatting dates\n",
    "import seaborn as sns # Visualization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch # Library for implementing Deep Neural Network \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Apple.Inc Stock Data\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import date, timedelta, datetime \n",
    "\n",
    "end_date = date.today().strftime(\"%Y-%m-%d\") #end date for our data retrieval will be current date \n",
    "start_date = '1990-01-01' # Beginning date for our historical data retrieval \n",
    "\n",
    "df = yf.download('AAPL', start=start_date, end=end_date)# Function used to fetch the data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_plot(df):\n",
    "\tdf_plot = df.copy()\n",
    "\n",
    "\tncols = 2\n",
    "\tnrows = int(round(df_plot.shape[1] / ncols, 0))\n",
    "\n",
    "\tfig, ax = plt.subplots(nrows=nrows, ncols=ncols,\n",
    "\t\t\t\t\t\tsharex=True, figsize=(14, 7))\n",
    "\tfor i, ax in enumerate(fig.axes):\n",
    "\t\tsns.lineplot(data=df_plot.iloc[:, i], ax=ax)\n",
    "\t\tax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
    "\t\tax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\tfig.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "data_plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split \n",
    "# Setting 80 percent data for training\n",
    "training_data_len = math.ceil(len(df) * .8)\n",
    "training_data_len\n",
    "\n",
    "#Splitting the dataset\n",
    "train_data = df[:training_data_len].iloc[:,:1]\n",
    "test_data = df[training_data_len:].iloc[:,:1]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Open Price values\n",
    "dataset_train = train_data.Open.values\n",
    "# Reshaping 1D to 2D array\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaling dataset\n",
    "scaled_train = scaler.fit_transform(dataset_train)\n",
    "\n",
    "print(scaled_train[:5])\n",
    "# Normalizing values between 0 and 1\n",
    "scaled_test = scaler.fit_transform(dataset_test)\n",
    "print(*scaled_test[:5]) #prints the first 5 rows of scaled_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data are transformed into a supervised learning problem that may be used to develop the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and labels for training data\n",
    "sequence_length = 50 # Number of time steps to look back\n",
    "X_train, y_train = [], []\n",
    "for i in range(len(scaled_train) - sequence_length):\n",
    "\tX_train.append(scaled_train[i:i+sequence_length])\n",
    "\ty_train.append(scaled_train[i+1:i+sequence_length+1])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences and labels for testing data\n",
    "sequence_length = 30 # Number of time steps to look back\n",
    "X_test, y_test = [], []\n",
    "for i in range(len(scaled_test) - sequence_length):\n",
    "\tX_test.append(scaled_test[i:i+sequence_length])\n",
    "\ty_test.append(scaled_test[i+1:i+sequence_length+1])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we defined a PyTorch network using LSTM architecture. The class consist of LSTM layer and linear layer. In LSTMModel class, we initialized parameters-\n",
    "\n",
    "input_size : number of features in the input data at each time step\n",
    "hidden_size : hidden units in LSTM layer\n",
    "num_layers : number of LSTM layers\n",
    "batch_first= True: input data will have the batch size as the first dimension\n",
    "The function super(LSTMModel, self).__init__() initializes the parent class for building the neural network.\n",
    "\n",
    "The forward method defines the forward pass of the model, where the input x is processed through the layers of the model to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\t# input_size : number of features in input at each time step\n",
    "\t# hidden_size : Number of LSTM units \n",
    "\t# num_layers : number of LSTM layers \n",
    "\tdef __init__(self, input_size, hidden_size, num_layers): \n",
    "\t\tsuper(LSTMModel, self).__init__() #initializes the parent class nn.Module\n",
    "\t\tself.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\t\tself.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "\tdef forward(self, x): # defines forward pass of the neural network\n",
    "\t\tout, _ = self.lstm(x)\n",
    "\t\tout = self.linear(out)\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PyTorch code, we need to check the hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the loss fuction to mean squared error. To optimize the parameters during the training, we have considered Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer \n",
    "\n",
    "Adaptive Moment Estimation is an algorithm for optimization technique for gradient descent. The method is really efficient when working with large problem involving a lot of data or parameters. It requires less memory and is efficient. Intuitively, it is a combination of the ‘gradient descent with momentum’ algorithm and the ‘RMSP’ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data Loader for batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_hist =[]\n",
    "test_hist =[]\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\ttotal_loss = 0.0\n",
    "\n",
    "\t# Training\n",
    "\tmodel.train()\n",
    "\tfor batch_X, batch_y in train_loader:\n",
    "\t\tbatch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\t\tpredictions = model(batch_X)\n",
    "\t\tloss = loss_fn(predictions, batch_y)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t# Calculate average training loss and accuracy\n",
    "\taverage_loss = total_loss / len(train_loader)\n",
    "\ttrain_hist.append(average_loss)\n",
    "\n",
    "\t# Validation on test data\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\ttotal_test_loss = 0.0\n",
    "\n",
    "\t\tfor batch_X_test, batch_y_test in test_loader:\n",
    "\t\t\tbatch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "\t\t\tpredictions_test = model(batch_X_test)\n",
    "\t\t\ttest_loss = loss_fn(predictions_test, batch_y_test)\n",
    "\n",
    "\t\t\ttotal_test_loss += test_loss.item()\n",
    "\n",
    "\t\t# Calculate average test loss and accuracy\n",
    "\t\taverage_test_loss = total_test_loss / len(test_loader)\n",
    "\t\ttest_hist.append(average_test_loss)\n",
    "\tif (epoch+1)%10==0:\n",
    "\t\tprint(f'Epoch [{epoch+1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,num_epochs,num_epochs)\n",
    "plt.plot(x,train_hist,scalex=True, label=\"Training loss\")\n",
    "plt.plot(x, test_hist, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the neural network on the provided data, now comes the forecasting for next month. The model predicts the future opening price and store the future values along with their corresponding dates. Using for loop, we are going to perform a rolling forecasting, the steps are as follows –\n",
    "\n",
    "We have set the future time steps to 30 and converted the test sequence to numpy array and remove singleton dimensions using sequence_to_plot.\n",
    "Then, we have converted historical_data to a Pytorch tensor. The shape of the tensor is (1, sequence_length, 1), where sequence_length is the length of the historical data sequence.\n",
    "the model further predicts the next value based on the ‘historical_data_tensor’.\n",
    "The prediction is then converted to a numpy array and the first element is extracted.\n",
    "Once the loop ends, we get the forecasted values, which are stored in list, and future dates are generated to create index for these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of future time steps to forecast\n",
    "num_forecast_steps = 30\n",
    "\n",
    "# Convert to NumPy and remove singleton dimensions\n",
    "sequence_to_plot = X_test.squeeze().cpu().numpy()\n",
    "\n",
    "# Use the last 30 data points as the starting point\n",
    "historical_data = sequence_to_plot[-1]\n",
    "print(historical_data.shape)\n",
    "\n",
    "# Initialize a list to store the forecasted values\n",
    "forecasted_values = []\n",
    "\n",
    "# Use the trained model to forecast future values\n",
    "with torch.no_grad():\n",
    "\tfor _ in range(num_forecast_steps*2):\n",
    "\t\t# Prepare the historical_data tensor\n",
    "\t\thistorical_data_tensor = torch.as_tensor(historical_data).view(1, -1, 1).float().to(device)\n",
    "\t\t# Use the model to predict the next value\n",
    "\t\tpredicted_value = model(historical_data_tensor).cpu().numpy()[0, 0]\n",
    "\n",
    "\t\t# Append the predicted value to the forecasted_values list\n",
    "\t\tforecasted_values.append(predicted_value[0])\n",
    "\n",
    "\t\t# Update the historical_data sequence by removing the oldest value and adding the predicted value\n",
    "\t\thistorical_data = np.roll(historical_data, shift=-1)\n",
    "\t\thistorical_data[-1] = predicted_value\n",
    "\n",
    "\t\t\n",
    "# Generate futute dates\n",
    "last_date = test_data.index[-1]\n",
    "\n",
    "# Generate the next 30 dates\n",
    "future_dates = pd.date_range(start=last_date + pd.DateOffset(1), periods=30)\n",
    "\n",
    "# Concatenate the original index with the future dates\n",
    "combined_index = test_data.index.append(future_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the size of the plot \n",
    "plt.rcParams['figure.figsize'] = [14, 4] \n",
    "\n",
    "\n",
    "#Test data\n",
    "plt.plot(test_data.index[-100:-30], test_data.Open[-100:-30], label = \"test_data\", color = \"b\") \n",
    "#reverse the scaling transformation\n",
    "original_cases = scaler.inverse_transform(np.expand_dims(sequence_to_plot[-1], axis=0)).flatten() \n",
    "\n",
    "#the historical data used as input for forecasting\n",
    "plt.plot(test_data.index[-30:], original_cases, label='actual values', color='green') \n",
    "\n",
    "#Forecasted Values \n",
    "#reverse the scaling transformation\n",
    "forecasted_cases = scaler.inverse_transform(np.expand_dims(forecasted_values, axis=0)).flatten() \n",
    "# plotting the forecasted values\n",
    "plt.plot(combined_index[-60:], forecasted_cases, label='forecasted values', color='red') \n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.title('Time Series Forecasting')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
