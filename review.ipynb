{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout, Embedding, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/ailaty/Desktop/Python Scripts/Amazon Product Review Sentiment Analysis using RNN-LSTM/AmazonReview.csv')\n",
    "\n",
    "# Printing shape of the dataset\n",
    "print(data.shape)\n",
    "# printing columns and rows information\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for NULL values\n",
    "print(\"Null Values:\\n\", data.isna().sum())\n",
    "\n",
    "# dropping null values\n",
    "data = data.dropna()\n",
    "\n",
    "# again checking for NULL values\n",
    "print(\"Null Values after dropping:\\n\", data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of unique values in Sentiment column\n",
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning: In this step, we will clean the ‘reviews.text’ column. We will remove the unwanted HTML tags, brackets, or special characters that may be present in the texts. We will use Regex to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading stopwords from nltk library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Review text Cleaning\n",
    "def clean_reviews(text):\n",
    "\t\n",
    "\t# removing html brackets and other square brackets from the string using regex\n",
    "\tregex = re.compile('<.*?>') # r'<.*?>'\n",
    "\ttext = re.sub(regex, '', text)\n",
    "\n",
    "\t# removing special characters like @, #, $, etc\n",
    "\tpattern = re.compile('[^a-zA-z0-9\\s]')\n",
    "\ttext = re.sub(pattern,'',text)\n",
    "\n",
    "\t# removing numbers\n",
    "\tpattern = re.compile('\\d+')\n",
    "\ttext = re.sub(pattern,'',text)\n",
    "\n",
    "\t# converting text to lower case\n",
    "\ttext = text.lower()\n",
    "\t\n",
    "\t# Tokenization of words\n",
    "\ttext = word_tokenize(text)\n",
    "\t\n",
    "\t# Stop words removal\n",
    "\ttext = [word for word in text if not word in stop_words]\n",
    "\t\n",
    "\treturn text\n",
    "\n",
    "# using the clean_reviews function on the dataset\n",
    "data['Review'] = data['Review'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# converting all the reviews to list to pass it as a parameter to fit_on_texts\n",
    "reviews_to_list = data['Review'].tolist()\n",
    "tokenizer.fit_on_texts(reviews_to_list)\n",
    "\n",
    "# Generating text sequences\n",
    "text_sequences = np.array(tokenizer.texts_to_sequences(reviews_to_list))\n",
    "\n",
    "# one hot encoding\n",
    "data = pd.get_dummies(data, columns = ['Sentiment'])\n",
    "\n",
    "# setting maximum words we want in an example\n",
    "max_words = 500\n",
    "\n",
    "# Generatin our X (input) to the model\n",
    "# using pad_sequences and y (output)\n",
    "X = pad_sequences(text_sequences, maxlen = max_words)\n",
    "y = data[['Sentiment_1', 'Sentiment_2', 'Sentiment_3', 'Sentiment_4',\n",
    "\t'Sentiment_5']]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a RNN model\n",
    "rnn = Sequential(name=\"Simple_RNN\")\n",
    "rnn.add(Embedding(len(tokenizer.word_index)+1,\n",
    "\t\t\t\t\t\tmax_words,\n",
    "\t\t\t\t\t\tinput_length=max_words))\n",
    "\n",
    "rnn.add(SimpleRNN(128,activation='relu',return_sequences=True))\n",
    "\n",
    "rnn.add(SimpleRNN(64,activation='relu',return_sequences=False))\n",
    "\n",
    "rnn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# printing model summary\n",
    "print(rnn.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
