{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "                  Open        High  ...   Adj Close    Volume\n",
      "Date                                ...                      \n",
      "2020-04-01   56.200001   56.471001  ...   55.105000  51970000\n",
      "2020-04-02   55.000000   56.138500  ...   55.851501  56410000\n",
      "...                ...         ...  ...         ...       ...\n",
      "2023-03-30  100.910004  101.160004  ...  100.889999  33086200\n",
      "2023-03-31  101.300003  103.889999  ...  103.730003  36863400\n",
      "\n",
      "[756 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#the start and end date\n",
    "start_date = dt.datetime(2020,4,1)\n",
    "end_date = dt.datetime(2023,4,1)\n",
    "\n",
    "#loading from yahoo finance\n",
    "data = yf.download(\"GOOGL\",start_date, end_date)\n",
    "\n",
    "pd.set_option('display.max_rows', 4)\n",
    "pd.set_option('display.max_columns',5)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 80 percent data for training\n",
    "training_data_len = math.ceil(len(data) * .8)\n",
    "training_data_len \n",
    "\n",
    "#Splitting the dataset\n",
    "train_data = data[:training_data_len].iloc[:,:1] \n",
    "test_data = data[training_data_len:].iloc[:,:1]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Open Price values\n",
    "dataset_train = train_data.Open.values \n",
    "# Reshaping 1D to 2D array\n",
    "dataset_train = np.reshape(dataset_train, (-1,1)) \n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaling dataset\n",
    "scaled_train = scaler.fit_transform(dataset_train)\n",
    "\n",
    "print(scaled_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Open Price values\n",
    "dataset_test = test_data.Open.values \n",
    "# Reshaping 1D to 2D array\n",
    "dataset_test = np.reshape(dataset_test, (-1,1)) \n",
    "# Normalizing values between 0 and 1\n",
    "scaled_test = scaler.fit_transform(dataset_test) \n",
    "print(*scaled_test[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(50, len(scaled_train)):\n",
    "\tX_train.append(scaled_train[i-50:i, 0])\n",
    "\ty_train.append(scaled_train[i, 0])\n",
    "\tif i <= 51:\n",
    "\t\tprint(X_train)\n",
    "\t\tprint(y_train)\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(50, len(scaled_test)):\n",
    "\tX_test.append(scaled_test[i-50:i, 0])\n",
    "\ty_test.append(scaled_test[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is converted to Numpy array\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "#Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "print(\"X_train :\",X_train.shape,\"y_train :\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is converted to numpy array\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "#Reshaping\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0],1))\n",
    "print(\"X_test :\",X_test.shape,\"y_test :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three RNN models are created in this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleRNN Model: \n",
    "Using the Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# adding RNN layers and dropout regularization\n",
    "regressor.add(SimpleRNN(units = 50, \n",
    "\t\t\t\t\t\tactivation = \"tanh\",\n",
    "\t\t\t\t\t\treturn_sequences = True,\n",
    "\t\t\t\t\t\tinput_shape = (X_train.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 50, \n",
    "\t\t\t\t\t\tactivation = \"tanh\",\n",
    "\t\t\t\t\t\treturn_sequences = True))\n",
    "\n",
    "regressor.add(SimpleRNN(units = 50,\n",
    "\t\t\t\t\t\tactivation = \"tanh\",\n",
    "\t\t\t\t\t\treturn_sequences = True))\n",
    "\n",
    "regressor.add( SimpleRNN(units = 50))\n",
    "\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units = 1,activation='sigmoid'))\n",
    "\n",
    "# compiling RNN\n",
    "regressor.compile(optimizer = SGD(learning_rate=0.01,\n",
    "\t\t\t\t\t\t\t\tdecay=1e-6, \n",
    "\t\t\t\t\t\t\t\tmomentum=0.9, \n",
    "\t\t\t\t\t\t\t\tnesterov=True), \n",
    "\t\t\t\tloss = \"mean_squared_error\")\n",
    "\n",
    "# fitting the model\n",
    "regressor.fit(X_train, y_train, epochs = 20, batch_size = 2)\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a LSTM Model with three layers and a dense output layer. It employs the optimizer as Adam, mean squared error as the loss function, and accuracy as the evaluation metric while compiling. With a batch size of 1, it fits the model to the training data for 10 epochs. The number of parameters in each layer and the overall number of parameters in the model are listed in a summary of the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the model\n",
    "regressorLSTM = Sequential()\n",
    "\n",
    "#Adding LSTM layers\n",
    "regressorLSTM.add(LSTM(50, \n",
    "\t\t\t\t\treturn_sequences = True, \n",
    "\t\t\t\t\tinput_shape = (X_train.shape[1],1)))\n",
    "regressorLSTM.add(LSTM(50, \n",
    "\t\t\t\t\treturn_sequences = False))\n",
    "regressorLSTM.add(Dense(25))\n",
    "\n",
    "#Adding the output layer\n",
    "regressorLSTM.add(Dense(1))\n",
    "\n",
    "#Compiling the model\n",
    "regressorLSTM.compile(optimizer = 'adam',\n",
    "\t\t\t\t\tloss = 'mean_squared_error',\n",
    "\t\t\t\t\tmetrics = [\"accuracy\"])\n",
    "\n",
    "#Fitting the model\n",
    "regressorLSTM.fit(X_train, \n",
    "\t\t\t\ty_train, \n",
    "\t\t\t\tbatch_size = 1, \n",
    "\t\t\t\tepochs = 12)\n",
    "regressorLSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) layer\n",
    "\n",
    "four stacked GRU layers followed by a single output layer\n",
    "\n",
    "It makes use of the ‘tanh’ hyperbolic tangent activation function. To avoid overfitting, a dropout layer with a rate of 0.2 is introduced.\n",
    "\n",
    "It employs the optimizer as Stochastic Gradient Descent (SGD) with a learning rate of 0.01, the decay rate of 1e-7, the momentum of 0.9, and Nesterov is set to False. The mean squared error is the loss function, and accuracy is the evaluation metric while compiling. With a batch size of 2, it fits the model to the training data for 20 epochs. The number of parameters in each layer and the overall number of parameters in the model are listed in a summary of the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the model\n",
    "regressorGRU = Sequential()\n",
    "\n",
    "# GRU layers with Dropout regularisation\n",
    "regressorGRU.add(GRU(units=50, \n",
    "\t\t\t\t\treturn_sequences=True,\n",
    "\t\t\t\t\tinput_shape=(X_train.shape[1],1),\n",
    "\t\t\t\t\tactivation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "\n",
    "regressorGRU.add(GRU(units=50, \n",
    "\t\t\t\t\treturn_sequences=True,\n",
    "\t\t\t\t\tactivation='tanh'))\n",
    "\n",
    "regressorGRU.add(GRU(units=50, \n",
    "\t\t\t\t\treturn_sequences=True,\n",
    "\t\t\t\t\tactivation='tanh'))\n",
    "\n",
    "regressorGRU.add(GRU(units=50, \n",
    "\t\t\t\t\tactivation='tanh'))\n",
    "\n",
    "# The output layer\n",
    "regressorGRU.add(Dense(units=1,\n",
    "\t\t\t\t\tactivation='relu'))\n",
    "# Compiling the RNN\n",
    "regressorGRU.compile(optimizer=SGD(learning_rate=0.01, \n",
    "\t\t\t\t\t\t\t\tdecay=1e-7, \n",
    "\t\t\t\t\t\t\t\tmomentum=0.9, \n",
    "\t\t\t\t\t\t\t\tnesterov=False),\n",
    "\t\t\t\t\tloss='mean_squared_error')\n",
    "\n",
    "# Fitting the data\n",
    "regressorGRU.fit(X_train,y_train,epochs=20,batch_size=1)\n",
    "regressorGRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with X_test data\n",
    "y_RNN = regressor.predict(X_test)\n",
    "y_LSTM = regressorLSTM.predict(X_test)\n",
    "y_GRU = regressorGRU.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values are transformed back from the normalized state to their original scale using the inverse_transform() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling back from 0-1 to original\n",
    "y_RNN_O = scaler.inverse_transform(y_RNN) \n",
    "y_LSTM_O = scaler.inverse_transform(y_LSTM) \n",
    "y_GRU_O = scaler.inverse_transform(y_GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,figsize =(18,12),sharex=True, sharey=True)\n",
    "fig.suptitle('Model Predictions')\n",
    "\n",
    "#Plot for RNN predictions\n",
    "axs[0].plot(train_data.index[150:], train_data.Open[150:], label = \"train_data\", color = \"b\")\n",
    "axs[0].plot(test_data.index, test_data.Open, label = \"test_data\", color = \"g\")\n",
    "axs[0].plot(test_data.index[50:], y_RNN_O, label = \"y_RNN\", color = \"brown\")\n",
    "axs[0].legend()\n",
    "axs[0].title.set_text(\"Basic RNN\")\n",
    "\n",
    "#Plot for LSTM predictions\n",
    "axs[1].plot(train_data.index[150:], train_data.Open[150:], label = \"train_data\", color = \"b\")\n",
    "axs[1].plot(test_data.index, test_data.Open, label = \"test_data\", color = \"g\")\n",
    "axs[1].plot(test_data.index[50:], y_LSTM_O, label = \"y_LSTM\", color = \"orange\")\n",
    "axs[1].legend()\n",
    "axs[1].title.set_text(\"LSTM\")\n",
    "\n",
    "#Plot for GRU predictions\n",
    "axs[2].plot(train_data.index[150:], train_data.Open[150:], label = \"train_data\", color = \"b\")\n",
    "axs[2].plot(test_data.index, test_data.Open, label = \"test_data\", color = \"g\")\n",
    "axs[2].plot(test_data.index[50:], y_GRU_O, label = \"y_GRU\", color = \"red\")\n",
    "axs[2].legend()\n",
    "axs[2].title.set_text(\"GRU\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
