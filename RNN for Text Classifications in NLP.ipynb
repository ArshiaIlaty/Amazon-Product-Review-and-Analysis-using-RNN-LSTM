{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the imdb review dataset from tensorflow datasets \n",
    "dataset = tfds.load('imdb_reviews', as_supervised=True) \n",
    "\n",
    "# Seperate test and train datasets \n",
    "train_dataset, test_dataset = dataset['train'], dataset['test'] \n",
    "\n",
    "# Split the test and train data into batches of 32 \n",
    "# and shuffling the training set \n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(10000) \n",
    "train_dataset = train_dataset.batch(batch_size) \n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example, label = next(iter(train_dataset)) \n",
    "print('Text:\\n', example.numpy()[0]) \n",
    "print('\\nLabel: ', label.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the TextVectorization layer to normalize, split, and map strings \n",
    "# to integers. \n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=10000) \n",
    "encoder.adapt(train_dataset.map(lambda text, _: text)) \n",
    "\n",
    "# Extracting the vocabulary from the TextVectorization layer. \n",
    "vocabulary = np.array(encoder.get_vocabulary()) \n",
    "\n",
    "# Encoding a test example and decoding it back. \n",
    "original_text = example.numpy()[0] \n",
    "encoded_text = encoder(original_text).numpy() \n",
    "decoded_text = ' '.join(vocabulary[encoded_text]) \n",
    "\n",
    "print('original: ', original_text) \n",
    "print('encoded: ', encoded_text) \n",
    "print('decoded: ', decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model \n",
    "model = tf.keras.Sequential([ \n",
    "\tencoder, \n",
    "\ttf.keras.layers.Embedding( \n",
    "\t\tlen(encoder.get_vocabulary()), 64, mask_zero=True), \n",
    "\ttf.keras.layers.Bidirectional( \n",
    "\t\ttf.keras.layers.LSTM(64, return_sequences=True)), \n",
    "\ttf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), \n",
    "\ttf.keras.layers.Dense(64, activation='relu'), \n",
    "\ttf.keras.layers.Dense(1) \n",
    "]) \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n",
    "\n",
    "# Compile the model \n",
    "model.compile( \n",
    "\tloss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "\toptimizer=tf.keras.optimizers.Adam(), \n",
    "\tmetrics=['accuracy'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model and validating it on test set \n",
    "history = model.fit( \n",
    "\ttrain_dataset, \n",
    "\tepochs=5, \n",
    "\tvalidation_data=test_dataset, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy and loss over time \n",
    "\n",
    "# Training history \n",
    "history_dict = history.history \n",
    "\n",
    "# Seperating validation and training accuracy \n",
    "acc = history_dict['accuracy'] \n",
    "val_acc = history_dict['val_accuracy'] \n",
    "\n",
    "# Seperating validation and training loss \n",
    "loss = history_dict['loss'] \n",
    "val_loss = history_dict['val_loss'] \n",
    "\n",
    "# Plotting \n",
    "plt.figure(figsize=(8, 4)) \n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(acc) \n",
    "plt.plot(val_acc) \n",
    "plt.title('Training and Validation Accuracy') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend(['Accuracy', 'Validation Accuracy']) \n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(loss) \n",
    "plt.plot(val_loss) \n",
    "plt.title('Training and Validation Loss') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.legend(['Loss', 'Validation Loss']) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "sample_text = ( \n",
    "\t'''The movie by GeeksforGeeks was so good and the animation are so dope. \n",
    "\tI would recommend my friends to watch it.'''\n",
    ") \n",
    "predictions = model.predict(np.array([sample_text])) \n",
    "print(*predictions[0]) \n",
    "\n",
    "# Print the label based on the prediction \n",
    "if predictions[0] > 0: \n",
    "\tprint('The review is positive') \n",
    "else: \n",
    "\tprint('The review is negative')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
